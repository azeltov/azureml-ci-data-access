{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Data on Compute Instance\n",
    "\n",
    "This notebook articulates how to access data residing on an Azure Cloud Datastore (Blob, ADLS gen2, SQL) in an AzureML compute instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import pandas as pd\n",
    "from azureml.core import Dataset, Workspace, Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Register the Datastore (if you haven't done so already)\n",
    "\n",
    "_Datastores_ are an AzureML abstraction for cloud-based stores of data - Blob, ADLS, SQL, Postgres. These are _registered_ into an AzureML workspace as a 'one-time' operation - typically at the start of a project. The benefit of the datastore abstraction is that all the credentials for the datastore are securely stored in an Azure Keyvault and do not have to be re-entered everytime you want to access some data on the store.\n",
    "\n",
    "An AzureML workspace comes with a default datastore (blob) named _workspaceblobstore_, however if your data resides in a different datastore, you can register it into the AzureML workspace. The easiest way to register a datastore is through the machine learning portal - https://ml.azure.com - go to __Datastores__ on the left-hand menu and then __+New Datastore__. Work through the on-screen instructions.\n",
    "\n",
    "### Step 2: Access Data\n",
    "\n",
    "Below enter the datastore name and the path on the datastore that you wish to create a dataset from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASTORE_NAME = \"\" # the default datastore is called workspaceblobstore.\n",
    "PATH_ON_DATASTORE = \"\"\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "my_datastore = Datastore.get(ws, DATASTORE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your data is tabular then start here\n",
    "\n",
    "_TabularDataset_ represents data in a tabular format created by parsing the provided file or list of files. Below we show how to create an AzureML Dataset object from the folder path that you defined above (we assume the data is in parquet) and then render the first 10 rows into a pandas dataframe. We also show how to read in data from delimited files and SQL -- uncomment the most appropriate one for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.Tabular.from_parquet_files(path = [(my_datastore, PATH_ON_DATASTORE)])\n",
    "dataset.take(10).to_pandas_dataframe()\n",
    "\n",
    "# to render the entire dataset into a pandas dataframe use:\n",
    "# df = dataset.to_pandas_dataframe()\n",
    "\n",
    "# to render into a spark dataframe use:\n",
    "# df = dataset.to_spark_dataframe()\n",
    "\n",
    "# to render a random sample of the dataset into a pandas dataframe use:\n",
    "# df = dataset.take_sample(probability=0.10).to_pandas_dataframe()\n",
    "\n",
    "# to create a dataset and pandas dataframe from delimited files use:\n",
    "# dataset = Dataset.Tabular.from_delimited_files(path = [(my_datastore, PATH_ON_DATASTORE)])\n",
    "# df = dataset.to_pandas_dataframe()\n",
    "# df.head(5) # show first 5 rows\n",
    "\n",
    "# to create a dataset and pandas dataframe from a sql query use:\n",
    "# q = \"\"\"SELECT * FROM TABLE\"\"\"\n",
    "# dataset = Dataset.Tabular.from_sql_query(q)\n",
    "# df = dataset.to_pandas_dataframe()\n",
    "# df.head(5) # show first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your data is file-based then start here\n",
    "_FileDataset_ references single or multiple files in datastores or from public URLs - an example may be a folder that contains a set of images that you want to train a deep-learning model on. Below we create a file dataset from the path that you defined above and then __mount__ the files onto the compute instance (into /tmp directory).\n",
    "\n",
    "The variable `mount_folder` is populated with the folder path of the mounted data. Below we use ```os.listdir(mount_folder)``` to display the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset.File.from_files(path=[(my_datastore, PATH_ON_DATASTORE)])\n",
    "\n",
    "mount_folder = tempfile.mkdtemp()\n",
    "mount_context = dset.mount(mount_folder)\n",
    "mount_context.start()\n",
    "\n",
    "print('files are mounted at: ' + mount_folder)\n",
    "os.listdir(mount_folder)\n",
    "\n",
    "\n",
    "# to unmount the folder from the compute instance use:\n",
    "# mount_context.stop()\n",
    "\n",
    "# to take a sample of files use:\n",
    "# mount_context.stop()\n",
    "# mount_context = dset.take(10).mount(mount_folder)\n",
    "# mount_context.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  From this point forward continue as normal\n",
    "\n",
    "From this point you can process your data just as you normally would on a local machine or server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
